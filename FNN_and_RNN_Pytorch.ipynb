{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e02e9498",
      "metadata": {
        "id": "e02e9498"
      },
      "source": [
        "# Sentiment Analysis Using Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5fdbd23",
      "metadata": {
        "id": "b5fdbd23"
      },
      "source": [
        "- Libraries Used:  __Pandas, gensim.downloader, Numpy, nltk, re, sklearn, torch__\n",
        "- Time to Run the code: __Approximately 17 minutes__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0c585e9",
      "metadata": {
        "id": "b0c585e9",
        "outputId": "a35d4956-32f1-441f-8d8f-7e8d28adac10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/hetvishah/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/hetvishah/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/hetvishah/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/hetvishah/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import gensim.downloader as api\n",
        "import contractions\n",
        "import gensim\n",
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "import numpy as np\n",
        "import nltk\n",
        "from numpy.linalg import norm\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83384e73",
      "metadata": {
        "id": "83384e73"
      },
      "source": [
        "### Task 1 - Dataset Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_vs8_4qbukp0"
      },
      "id": "_vs8_4qbukp0"
    },
    {
      "cell_type": "markdown",
      "id": "cfdbb596",
      "metadata": {
        "id": "cfdbb596"
      },
      "source": [
        "In the snippet below I load the dataset(available at [Amazon Reviews Dataset](https://www.kaggle.com/datasets/beaglelee/amazon-reviews-us-beauty-v1-00-tsv-zip)), assign them into classes and randomly extract a sample of data from each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e190e442",
      "metadata": {
        "id": "e190e442"
      },
      "outputs": [],
      "source": [
        "dataFile = pd.read_csv(r'data.tsv', sep='\\t', on_bad_lines = 'skip',low_memory=False)\n",
        "reducedDataFile = dataFile.filter(['review_body','review_headline','star_rating'])\n",
        "reducedDataFile = reducedDataFile[pd.to_numeric(reducedDataFile['star_rating'], errors='coerce').notnull()]\n",
        "reducedDataFile = reducedDataFile.astype({'star_rating':'int'})\n",
        "reducedDataFile = reducedDataFile.astype({'review_body':'str'})\n",
        "reducedDataFile = reducedDataFile.astype({'review_headline':'str'})\n",
        "def classes(x):\n",
        "    if x == 1 or x == 2:\n",
        "        temp = 1\n",
        "    elif x == 3:\n",
        "        temp = 2\n",
        "    else:\n",
        "        temp = 3\n",
        "    return temp\n",
        "tempClassList = []\n",
        "tempClassList = reducedDataFile['star_rating'].apply(classes)\n",
        "reviewDataFrame = reducedDataFile.assign(classes = tempClassList)\n",
        "reviewDataFrame = reviewDataFrame.groupby(['classes']).sample(n=20000)\n",
        "#reviewDataFrame.to_csv('storedDataset.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a01f5b4",
      "metadata": {
        "id": "2a01f5b4"
      },
      "source": [
        "### Task 2 - Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02906e05",
      "metadata": {
        "id": "02906e05"
      },
      "outputs": [],
      "source": [
        "#Loading the google pre-trained model\n",
        "loadPretrainedModel = api.load('word2vec-google-news-300')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c53473",
      "metadata": {
        "id": "e6c53473"
      },
      "source": [
        "### Task 2A - Check the Syntactic Understanding of the Pretrained Word2Vec Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24d4b5be",
      "metadata": {
        "id": "24d4b5be"
      },
      "source": [
        "To check the syntactic understanding of the model I take the following 3 examples:\n",
        "- 1) husband - man + woman = wife\n",
        "- 2) brilliant = superb\n",
        "- 3) beautiful = gorgeous"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1841464",
      "metadata": {
        "id": "b1841464"
      },
      "source": [
        "To find the similarity, I use cosine distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc3482e",
      "metadata": {
        "id": "efc3482e",
        "outputId": "f5ddd2a7-ceb6-4da5-cad8-7925ffd6ce71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1 Similarity:  0.7332699\n",
            "Example 2 Similarity:  0.7657863\n",
            "Example 3 Similarity:  0.8353004\n"
          ]
        }
      ],
      "source": [
        "#Semantic similarities of vectors\n",
        "\n",
        "#Example 1\n",
        "ex1_LHS = loadPretrainedModel['husband'] - loadPretrainedModel['man'] + loadPretrainedModel['woman']\n",
        "ex1_RHS = loadPretrainedModel['wife']\n",
        "sim1 = np.dot(ex1_LHS, ex1_RHS)/(norm(ex1_LHS)*norm(ex1_RHS))\n",
        "\n",
        "#Example 2\n",
        "ex2_LHS = loadPretrainedModel['brilliant']\n",
        "ex2_RHS = loadPretrainedModel['superb']\n",
        "sim2 = np.dot(ex2_LHS, ex2_RHS)/(norm(ex2_LHS)*norm(ex2_RHS))\n",
        "\n",
        "#Example 3\n",
        "ex3_LHS = loadPretrainedModel['beautiful']\n",
        "ex3_RHS = loadPretrainedModel['gorgeous']\n",
        "sim3 = np.dot(ex3_LHS, ex3_RHS)/(norm(ex3_LHS)*norm(ex3_RHS))\n",
        "\n",
        "print('Example 1 Similarity: ', sim1)\n",
        "print('Example 2 Similarity: ', sim2)\n",
        "print('Example 3 Similarity: ', sim3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4647fb05",
      "metadata": {
        "id": "4647fb05"
      },
      "source": [
        "### Task 2B - Building My Custom Word2Vec Model and Comparing Its Performance with the Pretrained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff1b132",
      "metadata": {
        "id": "bff1b132"
      },
      "outputs": [],
      "source": [
        "class MyCorpus:\n",
        "    def __iter__(self):\n",
        "        #data = pd.read_csv(r'storedDataset.csv',on_bad_lines = 'skip')\n",
        "        corpus = reviewDataFrame['review_body'].tolist()\n",
        "        for line in corpus:\n",
        "            line = str(line)\n",
        "            yield utils.simple_preprocess(line)\n",
        "\n",
        "\n",
        "sentences = MyCorpus()\n",
        "CustomModel = gensim.models.Word2Vec(sentences=sentences, vector_size=300,window=13, min_count=9)\n",
        "CustomModel.save(\"myCustomModel.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50c26cba",
      "metadata": {
        "id": "50c26cba",
        "outputId": "2c015cbb-530e-4fdb-9ca4-badada60b428"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1 Similarity:  0.83443147\n",
            "Example 2 Similarity:  0.3848225\n",
            "Example 3 Similarity:  0.8107748\n"
          ]
        }
      ],
      "source": [
        "#Semantic similarities of vectors\n",
        "\n",
        "#Example 1\n",
        "ex1_LHS = CustomModel.wv['husband'] - CustomModel.wv['man'] + CustomModel.wv['woman']\n",
        "ex1_RHS = CustomModel.wv['wife']\n",
        "sim1 = np.dot(ex1_LHS, ex1_RHS)/(norm(ex1_LHS)*norm(ex1_RHS))\n",
        "\n",
        "#Example 2\n",
        "ex2_LHS = CustomModel.wv['brilliant']\n",
        "ex2_RHS = CustomModel.wv['superb']\n",
        "sim2 = np.dot(ex2_LHS, ex2_RHS)/(norm(ex2_LHS)*norm(ex2_RHS))\n",
        "\n",
        "#Example 3\n",
        "ex3_LHS = CustomModel.wv['beautiful']\n",
        "ex3_RHS = CustomModel.wv['gorgeous']\n",
        "sim3 = np.dot(ex3_LHS, ex3_RHS)/(norm(ex3_LHS)*norm(ex3_RHS))\n",
        "\n",
        "print('Example 1 Similarity: ', sim1)\n",
        "print('Example 2 Similarity: ', sim2)\n",
        "print('Example 3 Similarity: ', sim3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90c86f02",
      "metadata": {
        "id": "90c86f02"
      },
      "source": [
        "### Conclusion\n",
        "To conclude, the pre-trained model performs better than the custom-trained model for the last two examples. However, for the first example, the custom-trained model shows better performance. I believe the performance depends on the dataset used. There might be cases where the words used in my examples are not present in the randomly sampled dataset, which could result in errors. The similarity performance can also vary with different datasets. I will use the pre-trained model further in my code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "788975b8",
      "metadata": {
        "id": "788975b8"
      },
      "source": [
        "### Task 3 - Comparing Sentiment Analysis Accuracy Using Simple Models: Perceptron and Linear SVC for Word2Vec and TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47af5106",
      "metadata": {
        "id": "47af5106"
      },
      "source": [
        "Before creating models, I will first clean the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "befdad8d",
      "metadata": {
        "id": "befdad8d"
      },
      "outputs": [],
      "source": [
        "#Convert reviews to lowercase\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x: x.lower())\n",
        "#Remove any data inside brackets\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x: re.sub('\\[.*?\\]', '', x))\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x: re.sub('\\(.*?\\)', '', x))\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x: re.sub('\\{.*?\\}', '', x))\n",
        "#Remove web URL's from the reviews\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x: re.sub('https:\\/\\/.*', '', x))\n",
        "#Remove HTML tags\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x: re.sub('<[^<]+?>', '', x))\n",
        "#Remove words with numericals\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x: re.sub('\\w*\\d\\w*', '', x))\n",
        "#Remove '\\n' from the text. I noticed some text containing them.\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x:  re.sub('\\n', '', x))\n",
        "#Fix words like couldn't -> could not using the contraction library\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x: contractions.fix(x))\n",
        "#Removing Alphanumeric characters\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
        "#Removing any extra space\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x: \" \".join(x.split()))\n",
        "#FOR REVIEW Headline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b682f5b5",
      "metadata": {
        "id": "b682f5b5"
      },
      "outputs": [],
      "source": [
        "#Lemmatization\n",
        "def getPosTag(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "def lemmatize(token):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokenizedList = pos_tag(word_tokenize(token))\n",
        "    lemmStr = ''\n",
        "    for val in tokenizedList:\n",
        "        Tag = getPosTag(val[1])\n",
        "        if Tag!= '':\n",
        "            lemmStr+= ' '+ lemmatizer.lemmatize(val[0], Tag)\n",
        "        else:\n",
        "            lemmStr+= ' '+ val[0]\n",
        "    return lemmStr\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x: lemmatize(x))\n",
        "\n",
        "\n",
        "#Remove tokens of length 1 or 2\n",
        "reviewDataFrame['review_body'] = reviewDataFrame['review_body'].apply(lambda x: ' '.join([word for word in x.split() if len(word) >= 3]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8e04918",
      "metadata": {
        "id": "c8e04918"
      },
      "outputs": [],
      "source": [
        "# Creating TF-IDF vectors\n",
        "trainingData, testingData , trainY, testY = train_test_split(reviewDataFrame['review_body'].values,reviewDataFrame['classes'].values,test_size=0.2,random_state=123, stratify = reviewDataFrame['classes'].values )\n",
        "vector = TfidfVectorizer()\n",
        "trainX = vector.fit_transform(trainingData)\n",
        "testX = vector.transform(testingData)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efd53fc0",
      "metadata": {
        "id": "efd53fc0"
      },
      "source": [
        "In order to create the word2Vec vectors, I first had to handle rows which were empty. For that, I replace any empty row with an <UNK> tag. And since, it is not presesnt in the google trained word2vec model, it will append zeros. Once these rows are handled, I get the summation of all word2Vec vectors for one review and divide it by the total review count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "519092f5",
      "metadata": {
        "id": "519092f5"
      },
      "outputs": [],
      "source": [
        "reviewDataFrame['review_body'].replace('', '<UNK>', inplace=True)\n",
        "\n",
        "#word2Vec Average vector\n",
        "word2Vec = []\n",
        "temp = np.zeros(300)\n",
        "for ele in reviewDataFrame['review_body'].values:\n",
        "    count = 0\n",
        "    temp = np.zeros(300)\n",
        "    stuff = ele.split(\" \")\n",
        "    for item in stuff:\n",
        "        count+=1\n",
        "        if item in loadPretrainedModel:\n",
        "            temp = np.add(temp,np.array(loadPretrainedModel[item]))\n",
        "    word2Vec.append(temp/count)\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05a70e1f",
      "metadata": {
        "id": "05a70e1f"
      },
      "source": [
        "Below, I split the data into training and testing sets with an 80% and 20% split, respectively. Then, I convert the input data (i.e., trainX and testX) into numpy arrays.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c7a7370",
      "metadata": {
        "id": "4c7a7370"
      },
      "outputs": [],
      "source": [
        "word2Vec_trainX, word2Vec_testX , word2Vec_trainY, word2Vec_testY = train_test_split(word2Vec,reviewDataFrame['classes'].values,test_size=0.2,random_state=123)\n",
        "word2Vec_trainX = np.array(word2Vec_trainX)\n",
        "word2Vec_testX = np.array(word2Vec_testX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aae6c20",
      "metadata": {
        "id": "7aae6c20",
        "outputId": "1223cf1c-8fb3-4b1c-b1c5-1d57558a2ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  60.34166666666667 %\n"
          ]
        }
      ],
      "source": [
        "#Accuracy on the Perceptron Model using TF-IDF\n",
        "modelPerceptron = Perceptron(random_state=0,alpha = 0.2,eta0 = 10,n_iter_no_change = 10,early_stopping=True)\n",
        "\n",
        "modelPerceptron.fit(trainX, trainY)\n",
        "accuracyPerceptron = modelPerceptron.score(testX,testY)\n",
        "print('Accuracy: ',accuracyPerceptron*100,'%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eef05520",
      "metadata": {
        "id": "eef05520",
        "outputId": "8917b242-e879-45af-ad85-89058f790a45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  47.333333333333336 %\n"
          ]
        }
      ],
      "source": [
        "#Accuracy on the Perceptron Model using Word2Vec\n",
        "\n",
        "modelPerceptron_word2Vec = Perceptron()\n",
        "modelPerceptron_word2Vec.fit(word2Vec_trainX, word2Vec_trainY)\n",
        "accuracyPerceptron_word2Vec = modelPerceptron_word2Vec.score(word2Vec_testX,word2Vec_testY)\n",
        "\n",
        "print('Accuracy: ',accuracyPerceptron_word2Vec*100,'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8bb89a5",
      "metadata": {
        "id": "c8bb89a5",
        "outputId": "b0a36f32-d28d-4416-b9bf-ce683f544fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  70.43333333333334 %\n"
          ]
        }
      ],
      "source": [
        "#Accuracy on the SVM Model using TF-IDF\n",
        "modelSVM = LinearSVC(loss  = 'hinge',tol = 1e-4, C = 0.7,intercept_scaling = 0.1, max_iter = 5000)\n",
        "\n",
        "modelSVM.fit(trainX, trainY)\n",
        "accuracySVM = modelSVM.score(testX,testY)\n",
        "print('Accuracy: ',accuracySVM*100,'%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9838bd",
      "metadata": {
        "id": "eb9838bd",
        "outputId": "686810fe-96b5-4c32-bbeb-e9232c00f7a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  65.06666666666666 %\n"
          ]
        }
      ],
      "source": [
        "#Accuracy on the SVM Model using word2Vec\n",
        "modelSVM_word2Vec = LinearSVC()\n",
        "\n",
        "modelSVM_word2Vec.fit(word2Vec_trainX, word2Vec_trainY)\n",
        "accuracySVM_word2Vec = modelSVM_word2Vec.score(word2Vec_testX,word2Vec_testY)\n",
        "print('Accuracy: ',accuracySVM_word2Vec*100,'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "333b97a6",
      "metadata": {
        "id": "333b97a6"
      },
      "source": [
        "### Conclusion\n",
        "In conclusion TFIDF performs significantly better than the word2Vec model for Perceptron. For SVM, the difference is relatively less, but TFIDF still performs better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1c3d3e",
      "metadata": {
        "id": "5b1c3d3e"
      },
      "source": [
        "### Task 4 - Feedforward Neural Network for Sentiment Analysis Using Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a9fa75c",
      "metadata": {
        "id": "8a9fa75c"
      },
      "source": [
        "The Data class converts the inputs and outputs for the model into tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de4bf944",
      "metadata": {
        "id": "de4bf944"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "class Data(Dataset):\n",
        "    def __init__(self, X_train, y_train):\n",
        "        self.X = torch.from_numpy(X_train.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y_train)\n",
        "        self.len = self.X.shape[0]\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ab8a666",
      "metadata": {
        "id": "9ab8a666"
      },
      "source": [
        "The following function first converts the output data into one-hot encoded vectors and then passes the training and testing data to the `Data` class defined above. It returns a concatenated tensor for the training and testing data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c449789d",
      "metadata": {
        "id": "c449789d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ConvertIntoTensor(word2Vec_trainX,word2Vec_trainY,word2Vec_testX,word2Vec_testY):\n",
        "    encoding = preprocessing.OneHotEncoder()\n",
        "    MLP_trainX = word2Vec_trainX\n",
        "    MLP_trainY = encoding.fit_transform(word2Vec_trainY.reshape(-1,1)).toarray()\n",
        "    MLP_testX =  word2Vec_testX\n",
        "    MLP_testY = encoding.fit_transform(word2Vec_testY.reshape(-1,1)).toarray()\n",
        "    MLPTrainData = Data(MLP_trainX,MLP_trainY)\n",
        "    MLPTestData = Data(MLP_testX,MLP_testY)\n",
        "    return MLPTrainData,MLPTestData\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2ae7e04",
      "metadata": {
        "id": "c2ae7e04"
      },
      "source": [
        "The following function first shuffles the dataset and converts a given percentage of the training data into validation data. It then returns the `train_loader` and `valid_loader`, which are passed to the models for training. The batch size, validation data size, and training data are passed as parameters to the function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee969bb8",
      "metadata": {
        "id": "ee969bb8"
      },
      "outputs": [],
      "source": [
        "def Dataloaders(batch_size,valid_size,MLPTrainData):\n",
        "    batch_size = batch_size\n",
        "    valid_size = valid_size\n",
        "\n",
        "    num_train = len(MLPTrainData)\n",
        "    indices = list(range(num_train))\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(MLPTrainData, batch_size=batch_size,\n",
        "                                               sampler=train_sampler)\n",
        "    valid_loader = torch.utils.data.DataLoader(MLPTrainData, batch_size=batch_size,\n",
        "                                               sampler=valid_sampler)\n",
        "    return train_loader,valid_loader\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8fc231c",
      "metadata": {
        "id": "f8fc231c"
      },
      "source": [
        "This is the main model. It has two hidden layers with 100 and 10 nodes, respectively.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f953b5a3",
      "metadata": {
        "id": "f953b5a3"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        hidden_1 = 100\n",
        "        hidden_2 = 10\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_1)\n",
        "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
        "        self.fc3 = nn.Linear(hidden_2, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8a491a6",
      "metadata": {
        "id": "c8a491a6"
      },
      "source": [
        "Here, I am training the model using PyTorch functionalities. For every batch in the `train_loader`, we first run a forward pass, calculate the loss, and run a backward pass for weight optimization. Once done, the model is evaluated on the validation data. If the loss for the current validation pass is less than the previous pass, the model is saved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53abaa0b",
      "metadata": {
        "id": "53abaa0b"
      },
      "outputs": [],
      "source": [
        "def trainEpoch(n_epochs,model,train_loader,valid_loader):\n",
        "    n_epochs = n_epochs\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            train_loss = criterion(output, target)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        for data, target in valid_loader:\n",
        "            output = model(data)\n",
        "            valid_loss = criterion(output, target)\n",
        "\n",
        "\n",
        "\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch+1,\n",
        "            train_loss.item(),\n",
        "            valid_loss.item()\n",
        "            ))\n",
        "\n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss.item()))\n",
        "            torch.save(model.state_dict(), 'model.pt')\n",
        "            valid_loss_min = valid_loss.item()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aa434df",
      "metadata": {
        "id": "1aa434df"
      },
      "source": [
        "Here, to get the predicted values, I pass the test data (i.e., dataloader) into the model, and it returns the predicted values. Since the output dimension is 3, I take the maximum value from each output and return the `prediction_list`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d720851",
      "metadata": {
        "id": "7d720851"
      },
      "outputs": [],
      "source": [
        "def predict(model, dataloader):\n",
        "    prediction_list = []\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        batch = batch[0].clone().detach().requires_grad_(True)\n",
        "        outputs = model(batch)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        prediction_list.append(predicted.cpu())\n",
        "    return prediction_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3119b57",
      "metadata": {
        "id": "e3119b57"
      },
      "outputs": [],
      "source": [
        "# Defining the model parameters\n",
        "input_dim = 300\n",
        "output_dim = 3\n",
        "batchSize = 32\n",
        "validationSize = 0.2\n",
        "n_epochs = 25\n",
        "learningRate = 0.003\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0beaabc",
      "metadata": {
        "id": "a0beaabc",
        "outputId": "a5a0f697-f30b-4973-e94b-2e54ae220b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.843965 \tValidation Loss: 0.950183\n",
            "Validation loss decreased (inf --> 0.950183).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.565988 \tValidation Loss: 0.850802\n",
            "Validation loss decreased (0.950183 --> 0.850802).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.903665 \tValidation Loss: 0.936189\n",
            "Epoch: 4 \tTraining Loss: 0.816334 \tValidation Loss: 0.714347\n",
            "Validation loss decreased (0.850802 --> 0.714347).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.832102 \tValidation Loss: 0.877351\n",
            "Epoch: 6 \tTraining Loss: 0.670143 \tValidation Loss: 0.836814\n",
            "Epoch: 7 \tTraining Loss: 0.685039 \tValidation Loss: 0.635168\n",
            "Validation loss decreased (0.714347 --> 0.635168).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.845585 \tValidation Loss: 0.696973\n",
            "Epoch: 9 \tTraining Loss: 0.922382 \tValidation Loss: 0.667591\n",
            "Epoch: 10 \tTraining Loss: 0.734397 \tValidation Loss: 0.779530\n",
            "Epoch: 11 \tTraining Loss: 0.714184 \tValidation Loss: 0.794143\n",
            "Epoch: 12 \tTraining Loss: 0.634325 \tValidation Loss: 0.657307\n",
            "Epoch: 13 \tTraining Loss: 0.539466 \tValidation Loss: 0.666344\n",
            "Epoch: 14 \tTraining Loss: 0.639923 \tValidation Loss: 0.413669\n",
            "Validation loss decreased (0.635168 --> 0.413669).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.712879 \tValidation Loss: 0.692783\n",
            "Epoch: 16 \tTraining Loss: 0.664730 \tValidation Loss: 0.913843\n",
            "Epoch: 17 \tTraining Loss: 0.826648 \tValidation Loss: 0.583851\n",
            "Epoch: 18 \tTraining Loss: 0.710932 \tValidation Loss: 0.930824\n",
            "Epoch: 19 \tTraining Loss: 0.671100 \tValidation Loss: 0.549391\n",
            "Epoch: 20 \tTraining Loss: 0.618161 \tValidation Loss: 0.906729\n",
            "Epoch: 21 \tTraining Loss: 0.601983 \tValidation Loss: 1.115706\n",
            "Epoch: 22 \tTraining Loss: 0.557081 \tValidation Loss: 0.920348\n",
            "Epoch: 23 \tTraining Loss: 0.793392 \tValidation Loss: 0.702058\n",
            "Epoch: 24 \tTraining Loss: 0.663733 \tValidation Loss: 0.832514\n",
            "Epoch: 25 \tTraining Loss: 0.741835 \tValidation Loss: 0.721106\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Converting the test and train Data into Tensors\n",
        "MLP_trainData_4A,MLP_testData_4A = ConvertIntoTensor(word2Vec_trainX,word2Vec_trainY,word2Vec_testX,word2Vec_testY)\n",
        "# Using dataloader to get the train and validation data\n",
        "train_loader,valid_loader = Dataloaders(batchSize,validationSize,MLP_trainData_4A)\n",
        "# Initializing the model\n",
        "model = Net()\n",
        "# Defining the loss criteria\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= learningRate)\n",
        "# Training the model on train data and testing it on the validation data\n",
        "model = trainEpoch(n_epochs,model,train_loader,valid_loader)\n",
        "# Loading the model parameters\n",
        "model.load_state_dict(torch.load('model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b71654d",
      "metadata": {
        "id": "1b71654d"
      },
      "outputs": [],
      "source": [
        "# Using DataLoaders to get the test_loader to test upon\n",
        "test_loader = torch.utils.data.DataLoader(MLP_testData_4A, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "165eef49",
      "metadata": {
        "id": "165eef49"
      },
      "outputs": [],
      "source": [
        "# predicting values on the test data\n",
        "predictions = predict(model,test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c6c1e26",
      "metadata": {
        "id": "0c6c1e26",
        "outputId": "180308a7-44a6-4cf6-c8a9-8daab5632036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  66.60833333333333 %\n"
          ]
        }
      ],
      "source": [
        "right = 0\n",
        "predictionsFinal = []\n",
        "for ele in list(predictions):\n",
        "    predictionsFinal.append(int(ele))\n",
        "for ele1,ele2 in zip(word2Vec_testY,predictionsFinal):\n",
        "    if ele1 == (ele2+1):\n",
        "        right+=1\n",
        "Accuracy_4A = (right/len(predictionsFinal))*100\n",
        "print('Accuracy: ',Accuracy_4A ,'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba2d2d9",
      "metadata": {
        "id": "aba2d2d9"
      },
      "source": [
        "### Task 4B - Concatenate the first 10 Word2Vec vectors for each review as the input feature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f83eb0a8",
      "metadata": {
        "id": "f83eb0a8"
      },
      "source": [
        "Here, I am creating the Word2Vec data as mentioned in the assignment. For every review, I take the first 10 words and concatenate their vectors. For any word not in the pre-trained Google model, I append zeros. For reviews shorter than 10 words, I pad with zeros so that the total array length becomes 3000.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa12732c",
      "metadata": {
        "id": "aa12732c"
      },
      "outputs": [],
      "source": [
        "word2Vec_4B = []\n",
        "\n",
        "for ele in reviewDataFrame['review_body'].values:\n",
        "    count = 0\n",
        "    temp = []\n",
        "    splitData = ele.split(\" \")\n",
        "    for item in splitData:\n",
        "        count+=1\n",
        "        if item in loadPretrainedModel:\n",
        "            #If word is present in the load-pretrained model\n",
        "            if count == 1:\n",
        "                # For the first word\n",
        "                temp = np.array(loadPretrainedModel[item])\n",
        "            else:\n",
        "                temp = np.concatenate((temp,np.array(loadPretrainedModel[item])))\n",
        "        else:\n",
        "            # If word is not present in the load-pretrained model\n",
        "            if count == 1:\n",
        "                # For the first word\n",
        "                temp = np.array(np.zeros(300))\n",
        "            else:\n",
        "                temp = np.concatenate((temp,np.zeros(300)))\n",
        "        if count == 10:\n",
        "            break\n",
        "    # padding with zeroes if the length of review is less than 10.\n",
        "    if count!= 10:\n",
        "        for countVal in range(count+1,11):\n",
        "            temp = np.concatenate((temp,np.zeros(300)))\n",
        "    word2Vec_4B.append(temp)\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c9ff4db",
      "metadata": {
        "id": "3c9ff4db"
      },
      "outputs": [],
      "source": [
        "#Splitting into train and test data - 80% and 20% respectively\n",
        "word2Vec_trainX_4B, word2Vec_testX_4B , word2Vec_trainY_4B, word2Vec_testY_4B = train_test_split(word2Vec_4B,reviewDataFrame['classes'].values,test_size=0.2,random_state=123)\n",
        "word2Vec_trainX_4B = np.array(word2Vec_trainX_4B)\n",
        "word2Vec_testX_4B = np.array(word2Vec_testX_4B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e818ac5c",
      "metadata": {
        "id": "e818ac5c"
      },
      "outputs": [],
      "source": [
        "# Defining model Paramters\n",
        "input_dim = 3000\n",
        "output_dim = 3\n",
        "batchSize = 25\n",
        "validationSize = 0.2\n",
        "n_epochs = 25\n",
        "learningRate = 0.008\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c9f3358",
      "metadata": {
        "id": "3c9f3358",
        "outputId": "fb1d33e7-4faa-4081-a8e1-6b81872b9250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.983894 \tValidation Loss: 0.928744\n",
            "Validation loss decreased (inf --> 0.928744).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.863392 \tValidation Loss: 0.867216\n",
            "Validation loss decreased (0.928744 --> 0.867216).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.835467 \tValidation Loss: 1.093449\n",
            "Epoch: 4 \tTraining Loss: 0.720671 \tValidation Loss: 1.274513\n",
            "Epoch: 5 \tTraining Loss: 0.588887 \tValidation Loss: 0.954057\n",
            "Epoch: 6 \tTraining Loss: 0.381003 \tValidation Loss: 0.963526\n",
            "Epoch: 7 \tTraining Loss: 0.617316 \tValidation Loss: 0.970150\n",
            "Epoch: 8 \tTraining Loss: 0.349621 \tValidation Loss: 1.275130\n",
            "Epoch: 9 \tTraining Loss: 0.341491 \tValidation Loss: 1.347012\n",
            "Epoch: 10 \tTraining Loss: 0.329319 \tValidation Loss: 1.154507\n",
            "Epoch: 11 \tTraining Loss: 0.269605 \tValidation Loss: 2.722850\n",
            "Epoch: 12 \tTraining Loss: 0.570581 \tValidation Loss: 1.789865\n",
            "Epoch: 13 \tTraining Loss: 0.211871 \tValidation Loss: 3.322661\n",
            "Epoch: 14 \tTraining Loss: 0.551105 \tValidation Loss: 2.264518\n",
            "Epoch: 15 \tTraining Loss: 0.157288 \tValidation Loss: 2.890789\n",
            "Epoch: 16 \tTraining Loss: 0.175560 \tValidation Loss: 2.558786\n",
            "Epoch: 17 \tTraining Loss: 0.281908 \tValidation Loss: 2.582681\n",
            "Epoch: 18 \tTraining Loss: 0.453036 \tValidation Loss: 2.081530\n",
            "Epoch: 19 \tTraining Loss: 0.125212 \tValidation Loss: 3.559738\n",
            "Epoch: 20 \tTraining Loss: 0.218788 \tValidation Loss: 1.982017\n",
            "Epoch: 21 \tTraining Loss: 0.181270 \tValidation Loss: 1.302142\n",
            "Epoch: 22 \tTraining Loss: 0.141156 \tValidation Loss: 2.949572\n",
            "Epoch: 23 \tTraining Loss: 0.428335 \tValidation Loss: 3.708028\n",
            "Epoch: 24 \tTraining Loss: 0.078155 \tValidation Loss: 2.689411\n",
            "Epoch: 25 \tTraining Loss: 0.190811 \tValidation Loss: 1.966248\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Converting the test and train Data into Tensors\n",
        "MLP_trainData_4B,MLP_testData_4B = ConvertIntoTensor(word2Vec_trainX_4B, word2Vec_trainY_4B , word2Vec_testX_4B, word2Vec_testY_4B)\n",
        "# Using dataloader to get the train and validation data\n",
        "train_loader_4B,valid_loader_4B = Dataloaders(batchSize,validationSize,MLP_trainData_4B)\n",
        "# Initializing the model\n",
        "model_4B = Net()\n",
        "# Loss Criteria\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_4B.parameters(), lr= learningRate)\n",
        "# Training epochs\n",
        "model_4B = trainEpoch(n_epochs,model_4B,train_loader_4B,valid_loader_4B)\n",
        "# Loading the model parameters\n",
        "model_4B.load_state_dict(torch.load('model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fef68bb",
      "metadata": {
        "id": "5fef68bb"
      },
      "outputs": [],
      "source": [
        "# Predicting values from the test data\n",
        "test_loader = torch.utils.data.DataLoader(MLP_testData_4B, batch_size=1)\n",
        "predictions = predict(model_4B,test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a84aae6",
      "metadata": {
        "id": "5a84aae6",
        "outputId": "df23e486-eae1-4e48-9251-4ea9405ef043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  57.01666666666667 %\n"
          ]
        }
      ],
      "source": [
        "# Checking the model Accuracy\n",
        "right = 0\n",
        "predictionsFinal = []\n",
        "for ele in list(predictions):\n",
        "    predictionsFinal.append(int(ele))\n",
        "for ele1,ele2 in zip(word2Vec_testY,predictionsFinal):\n",
        "    if ele1 == (ele2+1):\n",
        "        right+=1\n",
        "Accuracy_4B = right/len(predictionsFinal)*100\n",
        "print('Accuracy: ',Accuracy_4B ,'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc60dedc",
      "metadata": {
        "id": "bc60dedc"
      },
      "source": [
        "### Conclusion\n",
        "Considering the accuracy, the first model (which considers the review's average word2Vec) performs better than the later model(concatenating the first 10 review vectors). This should be because the first model can handle context better."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ebe906",
      "metadata": {
        "id": "a6ebe906"
      },
      "source": [
        "### Task 5 - Recurrent Neural Networks for Sentiment Analysis Using Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcdc5939",
      "metadata": {
        "id": "bcdc5939"
      },
      "source": [
        "### 5A - RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cc17e5",
      "metadata": {
        "id": "d2cc17e5"
      },
      "outputs": [],
      "source": [
        "# Defining an RNN module\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first = True)\n",
        "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "\n",
        "        out, _ = self.rnn(x,h0)\n",
        "        out = out.reshape(out.shape[0],-1)\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9accb043",
      "metadata": {
        "id": "9accb043"
      },
      "source": [
        "To create the Word2Vec model, I take the first 20 words from each review and add them to `word2Vec_5A`. I append zeros for any word not in the pre-trained Google model. For reviews shorter than 20 words, I pad them with zeros. Each row will have 20 vectors, with each vector having a length of 300. On the other hand, in an RNN, since we pass it word-by-word, it incorporates the context better and hence gives better accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d608181",
      "metadata": {
        "id": "1d608181"
      },
      "outputs": [],
      "source": [
        "def trainEpochforTask5(n_epochs,model,train_loader,valid_loader):\n",
        "    n_epochs = n_epochs\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            #hiddenState =  model.initHidden()[0]\n",
        "            data = data.squeeze(1)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            #target = target.reshape(np.array(target).shape[0],1)\n",
        "            train_loss = criterion(output, target)\n",
        "\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            #train_loss += loss.item()*data.size(0)\n",
        "\n",
        "        model.eval()\n",
        "        for data, target in valid_loader:\n",
        "            data = data.squeeze(1)\n",
        "\n",
        "            output = model(data)\n",
        "            #target = target.reshape(-1,1)\n",
        "            #loss = criterion(output, target)\n",
        "            valid_loss = criterion(output, target)\n",
        "            #valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "\n",
        "        #train_loss = train_loss/len(train_loader.dataset)\n",
        "        #valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch+1,\n",
        "            train_loss.item(),\n",
        "            valid_loss.item()\n",
        "            ))\n",
        "\n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss.item()))\n",
        "            torch.save(model.state_dict(), 'model.pt')\n",
        "            valid_loss_min = valid_loss.item()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "086e727f",
      "metadata": {
        "id": "086e727f"
      },
      "outputs": [],
      "source": [
        "word2Vec_5A = []\n",
        "for ele in reviewDataFrame['review_body'].values:\n",
        "    count = 0\n",
        "    temp = []\n",
        "    splitData = ele.split(\" \")\n",
        "    for item in splitData:\n",
        "        count+=1\n",
        "        if item in loadPretrainedModel:\n",
        "                temp.append(np.array(loadPretrainedModel[item]))\n",
        "        else:\n",
        "                temp.append(np.zeros(300))\n",
        "        if count == 20:\n",
        "            break\n",
        "    # Padding with zeros\n",
        "    if count!= 20:\n",
        "        for countVal in range(count+1,21):\n",
        "            temp.append(np.zeros(300))\n",
        "    word2Vec_5A.append(temp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f4bfb69",
      "metadata": {
        "id": "3f4bfb69"
      },
      "outputs": [],
      "source": [
        "#Splitting into train and test data - 80% and 20% respectively\n",
        "word2Vec_trainX_5A, word2Vec_testX_5A , word2Vec_trainY_5A, word2Vec_testY_5A = train_test_split(word2Vec_5A,reviewDataFrame['classes'].values,test_size=0.2,random_state=123)\n",
        "word2Vec_trainX_5A = np.array(word2Vec_trainX_5A)\n",
        "word2Vec_testX_5A = np.array(word2Vec_testX_5A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b45e03",
      "metadata": {
        "id": "80b45e03"
      },
      "outputs": [],
      "source": [
        "# Initializing the model parameters\n",
        "batchSize = 32\n",
        "validationSize = 0.2\n",
        "n_epochs = 25\n",
        "learningRate = 0.01\n",
        "# Converting the test and training data into tensors\n",
        "RNN_trainData_5A,RNN_testData_5A = ConvertIntoTensor(word2Vec_trainX_5A, word2Vec_trainY_5A , word2Vec_testX_5A, word2Vec_testY_5A)\n",
        "# Using dataloader to get the train and validation data\n",
        "train_loader_5A,valid_loader_5A = Dataloaders(batchSize,validationSize,RNN_trainData_5A)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b77f58c3",
      "metadata": {
        "id": "b77f58c3"
      },
      "source": [
        "Here, since each row has 20 different vectors of length 300, the input size is defined as 300. Additionally, because I convert the output data into one-hot encoded vectors, the output dimension for the RNN model will be 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def15dfc",
      "metadata": {
        "id": "def15dfc",
        "outputId": "4e498bf8-d260-419d-f9df-5d2eb46f8e9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 1.002603 \tValidation Loss: 1.114888\n",
            "Validation loss decreased (inf --> 1.114888).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 1.008980 \tValidation Loss: 1.113441\n",
            "Validation loss decreased (1.114888 --> 1.113441).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.748540 \tValidation Loss: 0.999444\n",
            "Validation loss decreased (1.113441 --> 0.999444).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.953780 \tValidation Loss: 0.981410\n",
            "Validation loss decreased (0.999444 --> 0.981410).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.830897 \tValidation Loss: 0.907053\n",
            "Validation loss decreased (0.981410 --> 0.907053).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.939435 \tValidation Loss: 0.888999\n",
            "Validation loss decreased (0.907053 --> 0.888999).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 1.037449 \tValidation Loss: 0.722463\n",
            "Validation loss decreased (0.888999 --> 0.722463).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.780343 \tValidation Loss: 0.756940\n",
            "Epoch: 9 \tTraining Loss: 0.956208 \tValidation Loss: 0.900624\n",
            "Epoch: 10 \tTraining Loss: 0.917627 \tValidation Loss: 0.767923\n",
            "Epoch: 11 \tTraining Loss: 0.901781 \tValidation Loss: 0.933924\n",
            "Epoch: 12 \tTraining Loss: 0.797621 \tValidation Loss: 0.855922\n",
            "Epoch: 13 \tTraining Loss: 0.909356 \tValidation Loss: 0.747029\n",
            "Epoch: 14 \tTraining Loss: 0.785776 \tValidation Loss: 0.844381\n",
            "Epoch: 15 \tTraining Loss: 0.666146 \tValidation Loss: 0.611397\n",
            "Validation loss decreased (0.722463 --> 0.611397).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.773906 \tValidation Loss: 0.812448\n",
            "Epoch: 17 \tTraining Loss: 0.777753 \tValidation Loss: 0.817422\n",
            "Epoch: 18 \tTraining Loss: 0.785289 \tValidation Loss: 0.714463\n",
            "Epoch: 19 \tTraining Loss: 0.635765 \tValidation Loss: 0.785454\n",
            "Epoch: 20 \tTraining Loss: 1.001419 \tValidation Loss: 0.726802\n",
            "Epoch: 21 \tTraining Loss: 1.005524 \tValidation Loss: 0.888917\n",
            "Epoch: 22 \tTraining Loss: 0.567937 \tValidation Loss: 0.861453\n",
            "Epoch: 23 \tTraining Loss: 0.722248 \tValidation Loss: 0.717454\n",
            "Epoch: 24 \tTraining Loss: 0.993305 \tValidation Loss: 0.766955\n",
            "Epoch: 25 \tTraining Loss: 0.700113 \tValidation Loss: 0.791372\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hiddenStateSize = 20\n",
        "sequence_length = 20\n",
        "hiddenLayers = 1\n",
        "# Initializing the model\n",
        "model_5A = RNN(300,hiddenStateSize,hiddenLayers,3)\n",
        "# Defining the loss Criteria\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_5A.parameters(), lr= learningRate)\n",
        "#training the model\n",
        "model_5A = trainEpochforTask5(n_epochs,model_5A,train_loader_5A,valid_loader_5A)\n",
        "# Loading the model parameters\n",
        "model_5A.load_state_dict(torch.load('model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b67b5a",
      "metadata": {
        "id": "90b67b5a"
      },
      "outputs": [],
      "source": [
        "# Loading and predicting on the test data\n",
        "test_loader = torch.utils.data.DataLoader(RNN_testData_5A, batch_size=1)\n",
        "predictions = predict(model_5A,test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63bca507",
      "metadata": {
        "id": "63bca507",
        "outputId": "71a76f00-f718-445e-d96b-ff5652bf90e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  61.00833333333333 %\n"
          ]
        }
      ],
      "source": [
        "#Checking model accuracy\n",
        "right = 0\n",
        "predictionsFinal = []\n",
        "for ele in list(predictions):\n",
        "    predictionsFinal.append(int(ele))\n",
        "for ele1,ele2 in zip(word2Vec_testY,predictionsFinal):\n",
        "    if ele1 == (ele2+1):\n",
        "        right+=1\n",
        "Accuracy_RNN = (right/len(predictionsFinal))*100\n",
        "print('Accuracy: ', Accuracy_RNN,'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fe9fe49",
      "metadata": {
        "id": "2fe9fe49"
      },
      "source": [
        "### Conclusion\n",
        "The Simple model performs better than RNN, since it considers the average, it can handle context better. On the other hand training an RNN renders a better accuracy as compared to 4B (Concatenating the first 10 reviews). Since in 4B we're simply concatenating the first 10 vectors, it might not be able to handle context well. On the other hand, In RNN, we send those words into the model one-by-one."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c42ed4f0",
      "metadata": {
        "id": "c42ed4f0"
      },
      "source": [
        "### 5B GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c6bb53",
      "metadata": {
        "id": "c6c6bb53"
      },
      "outputs": [],
      "source": [
        "# Defining a GRU model\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.GRU = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
        "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "\n",
        "        out, _ = self.GRU(x,h0)\n",
        "        out = out.reshape(out.shape[0],-1)\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3803649",
      "metadata": {
        "id": "b3803649"
      },
      "outputs": [],
      "source": [
        "#Splitting into train and test data - 80% and 20% respectively\n",
        "word2Vec_trainX_5B, word2Vec_testX_5B , word2Vec_trainY_5B, word2Vec_testY_5B = train_test_split(word2Vec_5A,reviewDataFrame['classes'].values,test_size=0.2,random_state=123)\n",
        "word2Vec_trainX_5B = np.array(word2Vec_trainX_5B)\n",
        "word2Vec_testX_5B = np.array(word2Vec_testX_5B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3506da5",
      "metadata": {
        "id": "b3506da5"
      },
      "outputs": [],
      "source": [
        "#defining model Parameters\n",
        "batchSize = 32\n",
        "validationSize = 0.2\n",
        "n_epochs = 25\n",
        "learningRate = 0.008\n",
        "#Converting into Tensors and loading data\n",
        "GRU_trainData_5B,GRU_testData_5B = ConvertIntoTensor(word2Vec_trainX_5B, word2Vec_trainY_5B , word2Vec_testX_5B, word2Vec_testY_5B)\n",
        "train_loader_5B,valid_loader_5B = Dataloaders(batchSize,validationSize,GRU_trainData_5B)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f0d5cb",
      "metadata": {
        "id": "18f0d5cb",
        "outputId": "b7cb6a2d-cb4a-45f6-b306-56a98ffecdf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 1.046179 \tValidation Loss: 1.062644\n",
            "Validation loss decreased (inf --> 1.062644).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.977733 \tValidation Loss: 1.026263\n",
            "Validation loss decreased (1.062644 --> 1.026263).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.819245 \tValidation Loss: 0.993745\n",
            "Validation loss decreased (1.026263 --> 0.993745).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.815836 \tValidation Loss: 0.864283\n",
            "Validation loss decreased (0.993745 --> 0.864283).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.720248 \tValidation Loss: 0.942974\n",
            "Epoch: 6 \tTraining Loss: 0.798292 \tValidation Loss: 0.890896\n",
            "Epoch: 7 \tTraining Loss: 1.124406 \tValidation Loss: 0.865597\n",
            "Epoch: 8 \tTraining Loss: 0.789420 \tValidation Loss: 0.857848\n",
            "Validation loss decreased (0.864283 --> 0.857848).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.858342 \tValidation Loss: 0.856926\n",
            "Validation loss decreased (0.857848 --> 0.856926).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.775309 \tValidation Loss: 0.993269\n",
            "Epoch: 11 \tTraining Loss: 0.688950 \tValidation Loss: 0.844262\n",
            "Validation loss decreased (0.856926 --> 0.844262).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.829468 \tValidation Loss: 0.840823\n",
            "Validation loss decreased (0.844262 --> 0.840823).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 0.726761 \tValidation Loss: 0.752854\n",
            "Validation loss decreased (0.840823 --> 0.752854).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.768638 \tValidation Loss: 0.931909\n",
            "Epoch: 15 \tTraining Loss: 0.834236 \tValidation Loss: 0.915177\n",
            "Epoch: 16 \tTraining Loss: 0.922842 \tValidation Loss: 0.840066\n",
            "Epoch: 17 \tTraining Loss: 0.861875 \tValidation Loss: 0.742683\n",
            "Validation loss decreased (0.752854 --> 0.742683).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.830027 \tValidation Loss: 0.790157\n",
            "Epoch: 19 \tTraining Loss: 0.802431 \tValidation Loss: 0.996688\n",
            "Epoch: 20 \tTraining Loss: 0.678496 \tValidation Loss: 0.622605\n",
            "Validation loss decreased (0.742683 --> 0.622605).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 0.778386 \tValidation Loss: 0.707523\n",
            "Epoch: 22 \tTraining Loss: 0.854845 \tValidation Loss: 0.724759\n",
            "Epoch: 23 \tTraining Loss: 0.847609 \tValidation Loss: 0.908594\n",
            "Epoch: 24 \tTraining Loss: 0.905796 \tValidation Loss: 0.861427\n",
            "Epoch: 25 \tTraining Loss: 0.629321 \tValidation Loss: 0.892275\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hiddenStateSize = 20\n",
        "sequence_length = 20\n",
        "hiddenLayers = 1\n",
        "# Initializing the model\n",
        "model_5B = GRU(300,hiddenStateSize,hiddenLayers,3)\n",
        "# Defining the loss criteria\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_5B.parameters(), lr= learningRate)\n",
        "# Training the model\n",
        "model_5B = trainEpochforTask5(n_epochs,model_5B,train_loader_5B,valid_loader_5B)\n",
        "# Loading the model parameters\n",
        "model_5B.load_state_dict(torch.load('model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad075097",
      "metadata": {
        "id": "ad075097"
      },
      "outputs": [],
      "source": [
        "# Loading and predicting on the test data\n",
        "test_loader = torch.utils.data.DataLoader(GRU_testData_5B, batch_size=1)\n",
        "predictions = predict(model_5B,test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57946252",
      "metadata": {
        "id": "57946252",
        "outputId": "551c7c74-6d20-4a9c-8fcf-7931f28055d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  62.6 %\n"
          ]
        }
      ],
      "source": [
        "#Checking the model accuracy\n",
        "right = 0\n",
        "predictionsFinal = []\n",
        "for ele in list(predictions):\n",
        "    predictionsFinal.append(int(ele))\n",
        "for ele1,ele2 in zip(word2Vec_testY,predictionsFinal):\n",
        "    if ele1 == (ele2+1):\n",
        "        right+=1\n",
        "Accuracy_GRU = (right/len(predictionsFinal))*100\n",
        "print('Accuracy: ',Accuracy_GRU , '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e244b92",
      "metadata": {
        "id": "0e244b92"
      },
      "source": [
        "### 5C LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a065844",
      "metadata": {
        "id": "5a065844"
      },
      "outputs": [],
      "source": [
        "# Defining an LSTM model\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.LSTM = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True)\n",
        "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        h1 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "\n",
        "        out, _ = self.LSTM(x,(h0,h1))\n",
        "        out = out.reshape(out.shape[0],-1)\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b750d8d",
      "metadata": {
        "id": "4b750d8d"
      },
      "outputs": [],
      "source": [
        "#Splitting into train and test data - 80% and 20% respectively\n",
        "word2Vec_trainX_5C, word2Vec_testX_5C , word2Vec_trainY_5C, word2Vec_testY_5C = train_test_split(word2Vec_5A,reviewDataFrame['classes'].values,test_size=0.2,random_state=123)\n",
        "word2Vec_trainX_5C = np.array(word2Vec_trainX_5C)\n",
        "word2Vec_testX_5C = np.array(word2Vec_testX_5C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00193a2c",
      "metadata": {
        "id": "00193a2c"
      },
      "outputs": [],
      "source": [
        "#Defining Model Paramters\n",
        "batchSize = 20\n",
        "validationSize = 0.2\n",
        "n_epochs = 25\n",
        "learningRate = 0.01\n",
        "#Converting into Tensors and loading data\n",
        "LSTM_trainData_5C,LSTM_testData_5C = ConvertIntoTensor(word2Vec_trainX_5C, word2Vec_trainY_5C , word2Vec_testX_5C, word2Vec_testY_5C)\n",
        "train_loader_5C,valid_loader_5C = Dataloaders(batchSize,validationSize,LSTM_trainData_5C)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d263cae9",
      "metadata": {
        "id": "d263cae9",
        "outputId": "72c7f2cf-9e5a-48d1-857c-69240158c76b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 1.129570 \tValidation Loss: 1.011557\n",
            "Validation loss decreased (inf --> 1.011557).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.923504 \tValidation Loss: 0.930789\n",
            "Validation loss decreased (1.011557 --> 0.930789).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.820379 \tValidation Loss: 0.930147\n",
            "Validation loss decreased (0.930789 --> 0.930147).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.773549 \tValidation Loss: 0.959385\n",
            "Epoch: 5 \tTraining Loss: 1.040869 \tValidation Loss: 0.955542\n",
            "Epoch: 6 \tTraining Loss: 0.997374 \tValidation Loss: 0.749450\n",
            "Validation loss decreased (0.930147 --> 0.749450).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.804810 \tValidation Loss: 0.834420\n",
            "Epoch: 8 \tTraining Loss: 0.877667 \tValidation Loss: 0.968348\n",
            "Epoch: 9 \tTraining Loss: 0.759487 \tValidation Loss: 1.031708\n",
            "Epoch: 10 \tTraining Loss: 0.930988 \tValidation Loss: 0.552397\n",
            "Validation loss decreased (0.749450 --> 0.552397).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.786059 \tValidation Loss: 0.941551\n",
            "Epoch: 12 \tTraining Loss: 0.776516 \tValidation Loss: 0.767472\n",
            "Epoch: 13 \tTraining Loss: 0.822038 \tValidation Loss: 1.292550\n",
            "Epoch: 14 \tTraining Loss: 0.968434 \tValidation Loss: 0.883535\n",
            "Epoch: 15 \tTraining Loss: 0.816811 \tValidation Loss: 1.072247\n",
            "Epoch: 16 \tTraining Loss: 0.946052 \tValidation Loss: 0.945146\n",
            "Epoch: 17 \tTraining Loss: 0.795354 \tValidation Loss: 0.528717\n",
            "Validation loss decreased (0.552397 --> 0.528717).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.798680 \tValidation Loss: 0.861694\n",
            "Epoch: 19 \tTraining Loss: 0.998954 \tValidation Loss: 0.922410\n",
            "Epoch: 20 \tTraining Loss: 0.883924 \tValidation Loss: 0.925207\n",
            "Epoch: 21 \tTraining Loss: 0.390672 \tValidation Loss: 0.683138\n",
            "Epoch: 22 \tTraining Loss: 0.777227 \tValidation Loss: 0.813926\n",
            "Epoch: 23 \tTraining Loss: 0.810441 \tValidation Loss: 0.746556\n",
            "Epoch: 24 \tTraining Loss: 0.658876 \tValidation Loss: 1.005438\n",
            "Epoch: 25 \tTraining Loss: 0.693659 \tValidation Loss: 0.617086\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hiddenStateSize = 20\n",
        "sequence_length = 20\n",
        "hiddenLayers = 1\n",
        "# Initializing the model\n",
        "model_5C = LSTM(300,hiddenStateSize,hiddenLayers,3)\n",
        "# Defining the loss criteria\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_5C.parameters(), lr= learningRate)\n",
        "# Training the model\n",
        "model_5C = trainEpochforTask5(n_epochs,model_5C,train_loader_5C,valid_loader_5C)\n",
        "# Loading model parameters\n",
        "model_5C.load_state_dict(torch.load('model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ce2f856",
      "metadata": {
        "id": "4ce2f856"
      },
      "outputs": [],
      "source": [
        "# Loading and predicting test Data\n",
        "test_loader = torch.utils.data.DataLoader(LSTM_testData_5C, batch_size=1)\n",
        "predictions = predict(model_5C,test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de5d1d0",
      "metadata": {
        "id": "9de5d1d0",
        "outputId": "bba78cee-7b0e-4225-cd44-f6bf38271daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  63.05 %\n"
          ]
        }
      ],
      "source": [
        "# Checking model accuracy\n",
        "right = 0\n",
        "predictionsFinal = []\n",
        "for ele in list(predictions):\n",
        "    predictionsFinal.append(int(ele))\n",
        "for ele1,ele2 in zip(word2Vec_testY,predictionsFinal):\n",
        "    if ele1 == (ele2+1):\n",
        "        right+=1\n",
        "Accuracy_LSTM = (right/len(predictionsFinal))*100\n",
        "print('Accuracy: ',Accuracy_LSTM ,'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "368e21c6",
      "metadata": {
        "id": "368e21c6"
      },
      "source": [
        "### Conclusion\n",
        "The performance of RNN, GRU and LSTM is almost on par with each other. LSTM performs slightly better than the other two. This would be because of the capability of an LSTM unit to handle the vanishing gradient problem. But overall, since the data is less, the performance is at a comparable level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a07b54",
      "metadata": {
        "id": "95a07b54",
        "outputId": "7681b6cb-c85f-44b6-8f93-17ba2a827f76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simple Models:  66.60833333333333 %\n",
            "4B:  57.01666666666667 %\n",
            "RNN:  61.00833333333333 %\n",
            "GRU:  62.6 %\n",
            "LSTM:  63.05 %\n"
          ]
        }
      ],
      "source": [
        "#Final Accuracies\n",
        "print('Simple Models: ',Accuracy_4A,'%' )\n",
        "print('4B: ',Accuracy_4B,'%' )\n",
        "print('RNN: ',Accuracy_RNN,'%' )\n",
        "print('GRU: ',Accuracy_GRU,'%' )\n",
        "print('LSTM: ',Accuracy_LSTM,'%' )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}