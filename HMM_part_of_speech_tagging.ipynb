{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39da04c",
   "metadata": {},
   "source": [
    "# Hidden Markov Model for Part of Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c877aebe",
   "metadata": {},
   "source": [
    "- Libraries Used:  __Pandas, Numpy, itertools, json__\n",
    "- Time to Run the code: __Approximately 7 minutes__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7818d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f06479",
   "metadata": {},
   "source": [
    "### Task 1 - Creating vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56875889",
   "metadata": {},
   "source": [
    "In the following cell, I am reading the tab seperated training data file. Since the file does not have a header, I convert the file into a dataframe with three columns namely: Index, Word and Tag. Next, I calucate and store the total number of sentences into a variable and am convert all the words into a numpy array which will further be used for vocabulary creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d3a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv('train', sep='\\t', header = None, names = ['Index','Word','Tag'])\n",
    "no_of_sentences = len(trainData[trainData['Index'] == 1])\n",
    "vocabularyCreation = np.array(trainData['Word']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531a261",
   "metadata": {},
   "source": [
    "The next step is to create the vocabulary. Here I have two dictionaries, one which calculates the occurances of every unique word in the training file and another for calculating the unknown word count. Later I append both these dictionary and write it to the output. The final dictionary is sorted lexicographically in descending order of its number of occurrences with the count of unknown words at the top as per the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d1b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Vocabulary\n",
    "wordFrequency = dict()\n",
    "unknownDict = dict()\n",
    "for ele in vocabularyCreation:\n",
    "    if ele not in wordFrequency.keys():\n",
    "        wordFrequency[ele] = 1\n",
    "    else:\n",
    "        wordFrequency[ele]+= 1\n",
    "\n",
    "#Sorting the dictionary lexicographically in descending order of its number of occurrences.\n",
    "wordFrequency = dict(sorted(wordFrequency.items(), key=lambda item: (-item[1],item[0])))\n",
    "unknownDict['<UNK>'] = 0\n",
    "for keys in wordFrequency.keys():\n",
    "    if wordFrequency[keys] < 2:\n",
    "        unknownDict['<UNK>'] += wordFrequency[keys]\n",
    "        wordFrequency[keys] = 0\n",
    "wordFrequency = {key:val for key, val in wordFrequency.items() if val != 0} \n",
    "#Appending the dictionaries\n",
    "wordFrequency = {**unknownDict, **wordFrequency}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb6e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the result to a file - vocab.txt\n",
    "i = 0\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for keys in wordFrequency.keys():\n",
    "        line = str(keys)+'\\t'+str(i)+'\\t'+str(wordFrequency[keys])+'\\n'\n",
    "        f.write(line)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd8952",
   "metadata": {},
   "source": [
    "RESULTS\n",
    "- Threshold for unknown words replacement: __2__\n",
    "- Total size of  vocabulary: __23182 words__\n",
    "- Total occurrences of the special token ‘< unk >’ after replacement: __20011__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b6eea",
   "metadata": {},
   "source": [
    "### Task 2  - Transmission and Emission Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ecbf0",
   "metadata": {},
   "source": [
    "In the following cell, I create a list for all the unique tags in our training data. I am also storing all consecutively occuring tags in the data as a list. Both these values are used to calculate the transmission probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d74140",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataTag =  list(trainData['Tag'])\n",
    "tags = list(trainData['Tag'].unique())\n",
    "trainingDataConsecutiveTags = list(zip(trainingDataTag, trainingDataTag[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9e19f",
   "metadata": {},
   "source": [
    "#### Transmission Probability calculation\n",
    "In order to efficienty calculate the transmission probabilities, I am creating three dictionaries:\n",
    "- A dictionary to store the count for each tag: tagOccurence\n",
    "- A dictionary to store the count for every consecutive tag: consecutiveTags\n",
    "- A dictionary to store the count for every starting tag: startCounts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2835fa19",
   "metadata": {},
   "source": [
    "I am iterating through the entire training data, which was stored as a list in our previous cell and storing the counts in the respective dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7df232",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagOccurence = dict()\n",
    "consecutiveTags = dict()\n",
    "startCounts = dict()\n",
    "\n",
    "#list of all starting tags\n",
    "startTags = trainData['Tag'][trainData['Index'] == 1]\n",
    "\n",
    "for ele in trainingDataTag:\n",
    "    if ele not in tagOccurence.keys():\n",
    "        tagOccurence[ele] = 1\n",
    "    else:\n",
    "        tagOccurence[ele] += 1\n",
    "        \n",
    "for ele in trainingDataConsecutiveTags:\n",
    "    if ele not in consecutiveTags.keys():\n",
    "        consecutiveTags[ele] = 1\n",
    "    else:\n",
    "        consecutiveTags[ele] += 1\n",
    "\n",
    "for ele in startTags:\n",
    "    if ('',ele) not in startCounts.keys():\n",
    "        startCounts[('',ele)] = 1\n",
    "    else:\n",
    "        startCounts[('',ele)] +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d4526e",
   "metadata": {},
   "source": [
    "Now to calculate the transmission probability, I am using the values previously stored in my dictionary. In total, there will be __((Total number of unique tags) * (Total number of unique tags) + 45 starting tags)__ elements in the transmission dictionary which accounts to __2070__. For this I use the inbuilt python function to get all the possible combination of tags. Since it does not consider (a,b) and (b,a) as two different tags, I invert the list and run the code again to account for every possible tag combinations. At the end, I count the probabilities for the start tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3839bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagsPossibleCombinations = list(itertools.combinations_with_replacement(tags,2))\n",
    "transmissionProbability = dict()\n",
    "\n",
    "#Transmission Prob for all unique tag combinations\n",
    "for ele in tagsPossibleCombinations:\n",
    "    if ele in consecutiveTags.keys():\n",
    "        transmissionProbability[ele] = (consecutiveTags[ele])/ (tagOccurence[ele[0]])\n",
    "    else:\n",
    "        transmissionProbability[ele] = 0\n",
    "invertTagsPossibleCombinations = []\n",
    "\n",
    "#Transmission Prob for the inverted tag list\n",
    "for ele in tagsPossibleCombinations:\n",
    "    invertTagsPossibleCombinations.append((ele[1],ele[0]))    \n",
    "for ele in invertTagsPossibleCombinations:\n",
    "    if ele in consecutiveTags.keys():\n",
    "        transmissionProbability[ele] = (consecutiveTags[ele])/ (tagOccurence[ele[0]])\n",
    "    else:\n",
    "        transmissionProbability[ele] = 0   \n",
    "        \n",
    "#Transmission Prob for start\n",
    "for ele in tags:\n",
    "    if ('',ele) in startCounts:\n",
    "        transmissionProbability[('',ele)] = (startCounts[('',ele)])/(no_of_sentences)\n",
    "    else:\n",
    "        transmissionProbability[('',ele)] = 0\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16591d4c",
   "metadata": {},
   "source": [
    "#### Emission Probability calculation\n",
    "I am using the same approach for calculation the emission probabilities. Here I have just one dictionary which stores all consecutive word counts in our training data. Here I also take into account the unknown word tags previously created in our vocab file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310ee739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A dict to store the counts for all (word, tag) pairs\n",
    "consecutiveEmissionTags = dict()\n",
    "emissionDF = trainData.drop(columns=[\"Index\"])\n",
    "emissionDataConsecutiveTags = emissionDF.values.tolist()\n",
    "index = -1\n",
    "for ele in emissionDataConsecutiveTags:\n",
    "    index+=1\n",
    "    if ele[0] not in wordFrequency.keys():\n",
    "        emissionDataConsecutiveTags[index][0] = '<UNK>'\n",
    "        \n",
    "for ele in emissionDataConsecutiveTags:\n",
    "    if tuple(ele) not in consecutiveEmissionTags.keys():\n",
    "        consecutiveEmissionTags[tuple(ele)] = 1\n",
    "    else:\n",
    "        consecutiveEmissionTags[tuple(ele)]+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3725fb",
   "metadata": {},
   "source": [
    "Once all the required values are stored in the dictionary, I use them to calculate the emission probabilities. The size of the dictionary will be __(number of unique vocabulary words)* number of unique tags__. I am using the python inbuilt itertool library to map all possible emission probability elements. Once mapped all the probabilities are stored in emissionProbability dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abfadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all possible emission pairs: vocabulary size*tags\n",
    "emissionProbability = dict()\n",
    "uniqueVocabularyWords = list(trainData['Word'].unique())\n",
    "index = -1\n",
    "for word in uniqueVocabularyWords:\n",
    "    index+=1\n",
    "    if word not in wordFrequency.keys():\n",
    "        uniqueVocabularyWords[index] = '<UNK>'\n",
    "        \n",
    "allPossibleEmissions = list(itertools.product(uniqueVocabularyWords, tags))\n",
    "for ele in allPossibleEmissions:\n",
    "    if ele in consecutiveEmissionTags.keys():\n",
    "        emissionProbability[ele] = (consecutiveEmissionTags[ele]/tagOccurence[ele[1]])\n",
    "    else:\n",
    "        emissionProbability[ele] = 0     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb25ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "transmissionProbabilityOutput = {\",\".join(key): value for key, value in transmissionProbability.items() if value !=0}\n",
    "emissionProbabilityOutput = {\",\".join(key): value for key, value in emissionProbability.items() if value != 0}\n",
    "\n",
    "hmmJSON = dict()\n",
    "hmmJSON['Transition'] = transmissionProbabilityOutput\n",
    "hmmJSON['Emission'] = emissionProbabilityOutput\n",
    "\n",
    "\n",
    "with open('hmm.json', 'w') as fp:\n",
    "    json.dump(hmmJSON, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b446a",
   "metadata": {},
   "source": [
    "#### RESULTS\n",
    "- Transition Probability Parameters: 2070\n",
    "- Emission Probability Parameters: 1043235\n",
    "- Transition Probability non-zero values: 1419\n",
    "- Emission Probability non-zero values: 30303"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55476751",
   "metadata": {},
   "source": [
    "NOTE: Whenever a start tag is encountered, I consider it as \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef534e",
   "metadata": {},
   "source": [
    "### Task 3 : Greedy Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e857257",
   "metadata": {},
   "source": [
    "For the greedy algorithm, I first train and evaluate my algorithm on the development dataset. For this, I extract the index,word pairs from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cbf2dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DevData = pd.read_csv('dev', sep='\\t', header = None, names = ['Index','Word','Tag'])\n",
    "trainingDevData = DevData.drop('Tag', axis = 1)\n",
    "trainingDevData = trainingDevData.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974ba3e",
   "metadata": {},
   "source": [
    "Here, I have defined my main function as: getGreedyTags. It takes in the (index,word) pair and returns the most appropriate tag as per definition. First of all, the greedy algorithm checks if the word in not present in our vocabulary and marks it as unknown. It then passes into a function defined as getMaxProb which calculates the most probable tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20a04e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxProb(index,word,tags,prevTag):\n",
    "    max = 0\n",
    "    tagname = ''\n",
    "    for tag in tags:\n",
    "        greedyProb = transmissionProbability[(prevTag,tag)] * emissionProbability[(word,tag)]\n",
    "        if greedyProb > max:\n",
    "            max = greedyProb\n",
    "            tagname = tag\n",
    "                \n",
    "    return tagname    \n",
    "\n",
    "def getGreedyTags(trainingDevData):\n",
    "    mySolution = []  \n",
    "    index = 0\n",
    "    for ele in trainingDevData:\n",
    "        if ele[1] not in uniqueVocabularyWords:\n",
    "            word = '<UNK>'\n",
    "        else:\n",
    "            word = ele[1]\n",
    "        if ele[0] == 1:\n",
    "            prevTag = ''\n",
    "        else:\n",
    "            prevTag = mySolution[-1]\n",
    "        prob = getMaxProb(ele[0],word,tags,prevTag)\n",
    "        mySolution.append(prob)\n",
    "        index+=1\n",
    "    return mySolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88929bc6",
   "metadata": {},
   "source": [
    "I then pass the development data into the function and check its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06e93097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.50069819683078\n"
     ]
    }
   ],
   "source": [
    "#Check Accuracy\n",
    "mySolution = getGreedyTags(trainingDevData)\n",
    "devSolution = DevData['Tag'].values.tolist()\n",
    "count = 0\n",
    "right = 0\n",
    "index = 0\n",
    "\n",
    "for a,b in zip(mySolution,devSolution):\n",
    "    count+=1\n",
    "    if a==b:\n",
    "        right+=1\n",
    "    index+=1\n",
    "print('Accuracy: ', (right/count)*100   )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3112a40",
   "metadata": {},
   "source": [
    "#### RESULTS:\n",
    "Accuracy on development Data : __93.5%__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00967ae",
   "metadata": {},
   "source": [
    "Predicting the test file Tags and writing them into greedy.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65221182",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestData = pd.read_csv('test', sep='\\t', header = None, names = ['Index','Word'])\n",
    "testDevData = TestData.values.tolist()\n",
    "testSolution = getGreedyTags(testDevData)\n",
    "greedyResults = zip(testDevData,testSolution)\n",
    "with open('greedy.out', 'w') as fileGreedy:\n",
    "    for keys in greedyResults:\n",
    "        line = str(keys[0][0])+'\\t'+str(keys[0][1])+'\\t'+str(keys[1])+'\\n'\n",
    "        if keys[0][0] == 1:\n",
    "            fileGreedy.write('\\n')\n",
    "        fileGreedy.write(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec83dbf",
   "metadata": {},
   "source": [
    "### Task 4: Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c01f46",
   "metadata": {},
   "source": [
    "For the viterbi algorithm, I start again with extracting and evaluating my algorithm on the development dataset. For this, I extract the index,word pairs from the dataset. I also keep a track of all the unique tags stored in trainViterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72344497",
   "metadata": {},
   "outputs": [],
   "source": [
    "DevDataViterbi = pd.read_csv('dev', sep='\\t', header = None, names = ['Index','Word','Tag'])\n",
    "trainingViterbi = DevDataViterbi.drop('Tag', axis = 1)\n",
    "trainingViterbi = trainingViterbi.values.tolist()\n",
    "tagViterbi = tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1273300",
   "metadata": {},
   "source": [
    "First of all, I convert all the unknown vocabulary words to <UNK>. Now for every sentence I have two main functionalities: buildMatrix and getParent.\n",
    "- Build Matrix : the function builds and returns a dynamic programming table with all the unique tags as its rows and all words in the sentence as it column. Every value in the table is stored as (Prob of(word,tag)pair, Parent)\n",
    "- getParent: The function backtracks the matrix and return the most efficient tag sequence for a word. One unique case which I encountered here was when all the previous columns were 0. In that case it assigns a tag '' to the word and goes to the prev column to find the maximum probable tag in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3721a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildMatrix(word,tagViterbi):\n",
    "    OPTMatrix = [[(0,'') for i in range(len(word))] for j in range(len(tagViterbi))]\n",
    "    for j in range(len(OPTMatrix[0])):\n",
    "        for i in range(len(OPTMatrix)): \n",
    "            if j==0:  \n",
    "                OPTMatrix[i][j] = (transmissionProbability[('',tagViterbi[i])] * emissionProbability[(word[j],tagViterbi[i])],'')\n",
    "                \n",
    "            else:         \n",
    "                maximum = 0\n",
    "                parentTag = ''\n",
    "                tempMax = 0\n",
    "                for allCol in range(len(OPTMatrix)):\n",
    "                    tempMax = OPTMatrix[allCol][j-1][0]*transmissionProbability[(tagViterbi[allCol],tagViterbi[i])] * emissionProbability[(word[j],tagViterbi[i])]                                                          \n",
    "                    if tempMax > maximum:\n",
    "                        maximum = tempMax\n",
    "                        parentTag = tagViterbi[allCol]\n",
    "                OPTMatrix[i][j] = (maximum,parentTag)\n",
    "                \n",
    "    return OPTMatrix    \n",
    "                    \n",
    "def getParent(OPTMatrix,tagViterbi):\n",
    "    viterbiTag = []\n",
    "    row = len(OPTMatrix)\n",
    "    col = len(OPTMatrix[0])\n",
    "    maximum = (0,'')\n",
    "    allZeros = True\n",
    "    for j in reversed(range(col)):    \n",
    "        if allZeros:\n",
    "            index = 0\n",
    "            for i in range(row):                  \n",
    "                if OPTMatrix[i][j][0] > maximum[0]:                    \n",
    "                    maximum = OPTMatrix[i][j]\n",
    "                    index = i\n",
    "            if maximum[0] > 0:\n",
    "                viterbiTag.append(tagViterbi[index])\n",
    "                allZeros = False\n",
    "                \n",
    "            else:\n",
    "                viterbiTag.append('')\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            if maximum[1] != '' :   \n",
    "                indexOfTag = tagViterbi.index(maximum[1])\n",
    "                viterbiTag.append(maximum[1])\n",
    "                maximum = OPTMatrix[indexOfTag][j]\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return viterbiTag       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a00f565",
   "metadata": {},
   "source": [
    "the function below takes in the training data, checks for unknown words, and returns a tag using the functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "275be63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getViterbiTags(trainingViterbi,uniqueVocabularyWords,tagViterbi):\n",
    "    viterbiTags = []\n",
    "    index = 0\n",
    "    if trainingViterbi[0][1] in uniqueVocabularyWords:\n",
    "        wordList = [trainingViterbi[0][1]]\n",
    "    else:\n",
    "        wordList = ['<UNK>']\n",
    "    for ele in trainingViterbi[1:]:\n",
    "        index+=1\n",
    "        if ele[1] not in uniqueVocabularyWords:\n",
    "            word = '<UNK>'\n",
    "        else:\n",
    "            word = ele[1]\n",
    "        if index == (len(trainingViterbi) - 1):\n",
    "            wordList.append(word)\n",
    "            \n",
    "        if ele[0] == 1 or index == (len(trainingViterbi) - 1):\n",
    "            OPTMatrix = buildMatrix(wordList,tagViterbi)\n",
    "            TagList = getParent(OPTMatrix,tagViterbi)\n",
    "            TagList = list(reversed(TagList))\n",
    "            for ele in TagList:\n",
    "                viterbiTags.append(ele)\n",
    "            wordList = [word]       \n",
    "        else:\n",
    "            wordList.append(word)\n",
    "    return viterbiTags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1932e2e4",
   "metadata": {},
   "source": [
    "Checking the development data accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb14d64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9477794305142372\n"
     ]
    }
   ],
   "source": [
    "viterbiTags = getViterbiTags(trainingViterbi,uniqueVocabularyWords,tagViterbi)\n",
    "devSolutionViterbi = DevDataViterbi['Tag'].values.tolist()\n",
    "count = 0\n",
    "right = 0\n",
    "\n",
    "for a,b in zip(viterbiTags,devSolutionViterbi):\n",
    "    count+=1\n",
    "    if a==b:\n",
    "        right+=1\n",
    "print('Accuracy: ', right/count) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa225048",
   "metadata": {},
   "source": [
    "#### RESULTS:\n",
    "Accuracy on development Data : __94.77%__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30901662",
   "metadata": {},
   "source": [
    "Predicting the test file Tags and writing them into viterbi.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "439549a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestDataViterbi = pd.read_csv('test', sep='\\t', header = None, names = ['Index','Word'])\n",
    "testDevDataViterbi = TestData.values.tolist()\n",
    "tagViterbi = tags\n",
    "viterbiTest = getViterbiTags(testDevDataViterbi,uniqueVocabularyWords,tagViterbi)\n",
    "viterbiResults = zip(testDevDataViterbi,viterbiTest)\n",
    "with open('viterbi.out', 'w') as fileViterbi:\n",
    "    for keys in viterbiResults:\n",
    "        line = str(keys[0][0])+'\\t'+str(keys[0][1])+'\\t'+str(keys[1])+'\\n'\n",
    "        if keys[0][0] == 1:\n",
    "            fileViterbi.write('\\n')\n",
    "        fileViterbi.write(line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
